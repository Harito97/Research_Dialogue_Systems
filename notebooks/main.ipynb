{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers datasets torch scikit-learn\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download dataset from HuggingFace\n",
    "def download_dataset():\n",
    "    dataset = load_dataset(\"pfb30/multi_woz_v22\", trust_remote_code=True)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Data preprocessing for (X, y)\n",
    "class MultiWozDataset(Dataset):\n",
    "    def __init__(self, dialogues, tokenizer, max_length=128):\n",
    "        self.inputs = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for dialogue in dialogues:\n",
    "            for turn in dialogue['turns']:\n",
    "                if 'dialogue_acts' in turn:\n",
    "                    act = list(turn['dialogue_acts'].keys())[0]  # First act type\n",
    "                    slots = [f\"{k}={v}\" for k, v in turn['dialogue_acts'][act][0]['slots']]\n",
    "                    label = f\"{act}|{'|'.join(slots)}\"\n",
    "                    self.inputs.append(turn['text'])\n",
    "                    self.labels.append(label)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.inputs[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoded = self.tokenizer(\n",
    "            input_text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
    "            'labels': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Model and training loop\n",
    "def train_model(train_loader, val_loader, num_labels, device='cuda'):\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(3):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss = {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        predictions, true_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                val_loss += criterion(logits, labels).item()\n",
    "\n",
    "                preds = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "                predictions.extend(preds)\n",
    "                true_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "        f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "        print(f\"Validation F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Testing\n",
    "def test_model(model, test_loader, device='cuda'):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1).cpu().tolist()\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    # Save predictions, true labels\n",
    "    with open('predictions.json', 'w') as f:\n",
    "        json.dump({'predictions': predictions, 'true_labels': true_labels}, f)\n",
    "    \n",
    "    # Load predictions, true labels\n",
    "    # with open('predictions.json', 'r') as f:\n",
    "    #     data = json.load(f)\n",
    "    #     predictions = data['predictions']\n",
    "    #     true_labels = data['true_labels']\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# # Step 1: Download dataset from HuggingFace\n",
    "# def download_dataset():\n",
    "#     from datasets import load_dataset\n",
    "#     dataset = load_dataset(\"pfb30/multi_woz_v22\", trust_remote_code=True)\n",
    "#     return dataset\n",
    "\n",
    "# # Step 2: Custom Dataset Class for MultiWoz\n",
    "# class MultiWozDataset(Dataset):\n",
    "#     def __init__(self, data, tokenizer, max_len=128):\n",
    "#         self.data = data\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_len = max_len\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data[\"utterance\"])\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         utterance = self.data[\"utterance\"][idx]\n",
    "#         labels = self.data[\"label\"][idx]\n",
    "\n",
    "#         # Tokenize input\n",
    "#         inputs = self.tokenizer(\n",
    "#             utterance, \n",
    "#             max_length=self.max_len, \n",
    "#             padding=\"max_length\", \n",
    "#             truncation=True, \n",
    "#             return_tensors=\"pt\"\n",
    "#         )\n",
    "#         inputs[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n",
    "#         return {key: val.squeeze(0) for key, val in inputs.items()}\n",
    "\n",
    "# # Step 3: Training Function\n",
    "# def train_model(train_loader, val_loader, num_labels, device):\n",
    "#     model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "#     model.to(device)\n",
    "\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "#     loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#     num_epochs = 3\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         train_loss = 0.0\n",
    "#         for batch in train_loader:\n",
    "#             inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
    "#             labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(**inputs)\n",
    "#             loss = loss_fn(outputs.logits, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             train_loss += loss.item()\n",
    "\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader)}\")\n",
    "\n",
    "#     # Validation Step\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in val_loader:\n",
    "#             inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
    "#             labels = batch[\"labels\"].to(device)\n",
    "#             outputs = model(**inputs)\n",
    "#             loss = loss_fn(outputs.logits, labels)\n",
    "#             val_loss += loss.item()\n",
    "\n",
    "#     print(f\"Validation Loss: {val_loss/len(val_loader)}\")\n",
    "#     return model\n",
    "\n",
    "# # Step 4: Testing Function\n",
    "# def test_model(model, test_loader, device):\n",
    "#     from sklearn.metrics import f1_score\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "\n",
    "#     all_preds, all_labels = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in test_loader:\n",
    "#             inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
    "#             labels = batch[\"labels\"].to(device)\n",
    "#             outputs = model(**inputs)\n",
    "#             preds = torch.argmax(outputs.logits, dim=-1)\n",
    "#             all_preds.extend(preds.cpu().numpy())\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "#     f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "#     print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Main Functions\n",
    "def main1():\n",
    "    print(\"Download dataset\")\n",
    "    dataset = download_dataset()\n",
    "    \n",
    "    print(\"Save dataset to disk\")\n",
    "    dataset_dict = {split: dataset[split].to_dict() for split in dataset}\n",
    "    with open('dataset_multiwoz.json', 'w') as f:\n",
    "        json.dump(dataset_dict, f)\n",
    "\n",
    "def main2():\n",
    "    print(\"Load dataset from disk\")\n",
    "    with open('dataset_multiwoz.json', 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    print(\"Tokenizer\")\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    print(\"Create train, val, test data\")\n",
    "    train_data = MultiWozDataset(dataset['train'], tokenizer)\n",
    "    val_data = MultiWozDataset(dataset['validation'], tokenizer)\n",
    "    test_data = MultiWozDataset(dataset['test'], tokenizer)\n",
    "\n",
    "    print(\"Save train, val, test data to disk\")\n",
    "    torch.save(train_data, 'train_data.pt')\n",
    "    torch.save(val_data, 'val_data.pt')\n",
    "    torch.save(test_data, 'test_data.pt')\n",
    "\n",
    "def main3():\n",
    "    print(\"Load train, val, test data from disk\")\n",
    "    train_data = torch.load('train_data.pt')\n",
    "    val_data = torch.load('val_data.pt')\n",
    "    test_data = torch.load('test_data.pt')\n",
    "\n",
    "    print(\"Create data loaders\")\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=32)\n",
    "    test_loader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "    print(\"Save data loaders to disk\")\n",
    "    torch.save(train_loader, 'train_loader.pt')\n",
    "    torch.save(val_loader, 'val_loader.pt')\n",
    "    torch.save(test_loader, 'test_loader.pt')\n",
    "\n",
    "def main4():\n",
    "    print(\"Load data loaders from disk\")\n",
    "    train_loader = torch.load('train_loader.pt')\n",
    "    val_loader = torch.load('val_loader.pt')\n",
    "\n",
    "    print(\"Train model\")\n",
    "    model = train_model(train_loader, val_loader, num_labels=100, device='cpu')\n",
    "    \n",
    "    print(\"Save model to disk\")\n",
    "    torch.save(model, 'model.pt')\n",
    "\n",
    "def main5():\n",
    "    print(\"Load model from disk\")\n",
    "    model = torch.load('model.pt')\n",
    "\n",
    "    print(\"Load test data loader\")\n",
    "    test_loader = torch.load('test_loader.pt')\n",
    "\n",
    "    print(\"Test model\")\n",
    "    test_model(model, test_loader, device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main1\n",
      "Download dataset\n",
      "Save dataset to disk\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    # print(\"Main1\")\n",
    "    # main1()\n",
    "    \n",
    "    print(\"Main2\")\n",
    "    main2()\n",
    "    \n",
    "    # print(\"Main3\")\n",
    "    # main3()\n",
    "    \n",
    "    # print(\"Main4\")\n",
    "    # main4()\n",
    "    \n",
    "    # print(\"Main5\")\n",
    "    # main5()\n",
    "    \n",
    "    print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main1\n",
    "Download dataset\n",
    "Save dataset to disk\n",
    "Done\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_work#311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
